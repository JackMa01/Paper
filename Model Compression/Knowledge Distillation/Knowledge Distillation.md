# Knowledge Distillation

## Response-Based

- Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." *arXiv preprint arXiv:1503.02531* 2.7 (2015).
  - [[Paper](https://openaccess.thecvf.com/content/WACV2022/supplemental/Jeong_BiHPF_Bilateral_High-Pass_WACV_2022_supplemental.zip)]
- Kim, Jangho, SeongUk Park, and Nojun Kwak. "Paraphrasing complex network: Network compression via factor transfer." *Advances in neural information processing systems* 31 (2018).
  - [[Abstract](https://proceedings.neurips.cc/paper/2018/hash/6d9cb7de5e8ac30bd5e8734bc96a35c1-Abstract.html)] [[Paper](https://proceedings.neurips.cc/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf)]
- Ba, Jimmy, and Rich Caruana. "Do deep nets really need to be deep?." *Advances in neural information processing systems* 27 (2014).
  - [[Abstract](https://proceedings.neurips.cc/paper/2014/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html)]  [[Paper](https://proceedings.neurips.cc/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf)]
- Mirzadeh, Seyed Iman, et al. "Improved knowledge distillation via teacher assistant." *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 34. No. 04. 2020.
  - [[Abstract](https://ojs.aaai.org/index.php/AAAI/article/view/5963)] [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/5963/5819)]
- Meng, Zhong, et al. "Conditional teacher-student learning." *ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2019.
  - [[Abstract](https://ieeexplore.ieee.org/abstract/document/8683438/)] [[Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8683438)]



## Feature-Based

- Chen, Yuntao, Naiyan Wang, and Zhaoxiang Zhang. "Darkrank: Accelerating deep metric learning via cross sample similarities transfer." *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 32. No. 1. 2018.
  - [[Abstract](https://ojs.aaai.org/index.php/AAAI/article/view/11783)] [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/11783/11642)] [[Note](https://github.com/JackMa01/PaperNote/blob/main/PaperNote.md#yuntao-chen-naiyan-wang-zhaoxiang-zhang-darkrank-accelerating-deep-metric-learning-via-cross-sample-similarities-transfer-in-aaai-2018)]



## Relation-Based

- Yim, Junho, et al. "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning." *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 2017.
  - [[Abstract](https://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html)] [[Paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)] [[Note](https://github.com/JackMa01/PaperNote/blob/main/PaperNote.md#junho-yim-donggyu-joo-jihoon-bae-junmo-kim-a-gift-from-knowledge-distillation-fast-optimization-network-minimization-and-transfer-learning-in-cvpr-2017)]

- Zagoruyko, Sergey, and Nikos Komodakis. "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer." *arXiv preprint arXiv:1612.03928* (2016).

  - [[Abstract](https://hal.archives-ouvertes.fr/hal-01832769/)] [[Paper](https://arxiv.org/pdf/1612.03928.pdf)] [[Note](https://github.com/JackMa01/PaperNote/blob/main/PaperNote.md#sergey-zagoruyko-nikos-komodakis-paying-more-attention-to-attention-improving-the-performance-of-convolutional-neural-networks-via-attention-transfer-in-iclr-2017)]

  
